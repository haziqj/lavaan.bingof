---
title: "Introduction"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    number_sections: FALSE
link-citations: yes
bibliography: refs.bib
pkgdown:
  as_is: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(lavaan.bingof)
```

```{r, echo = FALSE, results = "asis"}
# LaTeX shortcuts 
cat(readr::read_file("maths_shortcuts.tex"))
``` 

<!-- Extra LaTeX commands -->
\newcommand{\pimod}[1]{\pi_{#1}(\btheta)}
\newcommand{\Sigmaystar}{\bSigma_{\by^*}}
<!-- Extra LaTeX commands -->

Let $\bY = (Y_1, \dots, Y_p)^\top \in \{0,1\}^p$ be a vector of Bernoulli random variables. 
The probability of success for each Bernoulli component $Y_i$ is denoted $\dot\pi_i = \Pr(Y_i = 1)$, $i=1,\dots,p$.
Consider a response pattern $\by = (y_1,\dots,y_p)^\top$, where each $y_i\in\{0,1\}$.
The probability of observing such a response pattern is given by the joint distribution
\begin{align*}
\pi
&= \Pr(\bY = \by)  = \Pr(Y_1=y_1,\dots,Y_p=y_p).
\end{align*}
Note that there are a total of $R=2^p$ possible joint probabilities $\pi_r$ corresponding to all possible two-way response patterns $\by_r$.
When we consider a parametric model with parameter vector $\btheta\in\bbR^t$, we write $\pi_r(\btheta)$ to indicate each joint probability, and 
\begin{equation}
\bpi(\btheta) = \begin{pmatrix}
\pimod{1} \\
\vdots \\
\pimod{R}  \\
\end{pmatrix} \in [0,1]^R
(\#eq:jointprob)
\end{equation}
for the vector of joint probabilities, with $\sum_{r=1}^R \pimod{r} =1$.

# Model-based probabilities

The model of interest is a factor model, commonly used in social statistics.
Using an underlying variable (UV) approach, the observed binary responses $y_i$ are manifestations of some latent, continuous variables $Y_i^*$, $i=1,\dots,p$.
The connection is made as follows:
$$
Y_i = \begin{cases}
1 & Y_i^* > \tau_i \\
0 & Y_i^* \leq \tau_i,
\end{cases}
$$
where $\tau_i$ is the threshold associated with the variable $Y_i^*$.
For convenience, $Y_i^*$ is taken to be standard normal random variables^[For parameter identifiability, the location and scale of the normal distribution have to be fixed if the thresholds are to be estimated.].
The factor model takes the form
$$
\mathbf Y^* = \mathbf \Lambda\boldsymbol \eta + \boldsymbol \epsilon,
$$
where each component is explained below:

- $\mathbf Y^* = (Y_1^*,\dots,Y_p^*)^\top \in \mathbf R^p$ are the underlying variables;

- $\boldsymbol\Lambda \in \mathbf R^{p \times q}$ is the matrix of loadings;

- $\boldsymbol \eta = (\eta_1,\dots,\eta_q)^\top \in \mathbf R^q$ is the vector of latent factors;

- $\boldsymbol \epsilon \in \mathbf R^p$ are the error terms associated with the items (aka unique variables).

We also make some distributional assumptions, namely

1. $\boldsymbol\eta \sim \operatorname{N}_q(\mathbf 0, \boldsymbol\Psi)$, where $\Psi$ is a correlation matrix, i.e. for $k,l\in\{1,\dots,q\}$,
$$
\Psi_{kl} = \begin{cases}
1 & \text{if }  k = l \\
\rho(\eta_k, \eta_l) & \text{if } k \neq l.
\end{cases}
$$

2. $\boldsymbol \epsilon \sim \operatorname{N}_p(\mathbf 0, \boldsymbol\Theta_\epsilon)$, with $\boldsymbol\Theta_\epsilon = \mathbf I - \operatorname{diag}(\boldsymbol\Lambda \boldsymbol\Psi \boldsymbol\Lambda^\top)$.

These two assumptions, together with $\operatorname{Cov}(\boldsymbol\eta, \boldsymbol\epsilon) = 0$, implies that $\mathbf Y^*\sim\operatorname{N}_p(\mathbf 0,\Sigmaystar)$, where
\begin{align}
\Sigmaystar
= \operatorname{Var}(\mathbf Y^*) 
&= \boldsymbol\Lambda\boldsymbol\Phi\boldsymbol\Lambda^\top + \boldsymbol\Theta_\epsilon \\
&= \mathbf I + (\boldsymbol\Lambda\boldsymbol\Phi\boldsymbol\Lambda^\top - \operatorname{diag}\big(\boldsymbol\Lambda \boldsymbol\Phi \boldsymbol\Lambda^\top)\big).
\end{align}
<!-- Evidently, -->
<!-- $$ -->
<!-- \operatorname{Cor}(y_i^*, y_{i'}^*) = \sum_{k,l=1}^q \lambda_{ik}\lambda_{i'l} (1 + \rho(z_k,z_l)). -->
<!-- $$ -->
The parameter vector for this factor model is denoted $\boldsymbol\theta^\top = (\boldsymbol\lambda, \boldsymbol\phi, \boldsymbol\tau)$, where it contains the vectors of the free non-redundant parameters in $\boldsymbol\Lambda$ and $\boldsymbol \Psi$ respectively, as well as the vector of all free thresholds.

Under this factor model, the probability of response pattern $\mathbf y_r$ is
\begin{align}
\pi_r(\mathbf\theta) 
&= \Pr(\mathbf Y = \mathbf y_r \mid \boldsymbol\theta) \\
&= \idotsint_A f_p(\mathbf y^* \mid \mathbf 0, \boldsymbol\Sigma_{\mathbf y^*} ) \, \text{d}\mathbf y^*
\end{align}
where $f_p(\cdot \mid \boldsymbol\mu,\boldsymbol\Sigma)$ is the density function of the $p$-dimensional normal distribution with mean $\boldsymbol\mu$ and variance $\boldsymbol\Sigma$.
This integral is evaluated on the set
$$
A = \{ \mathbf Y^* \in \mathbb R^p \mid Y_1=y_1,\dots,Y_p=y_p \}.
$$


# Parameter estimation

Suppose that $h=1,\dots,n$ observations of $\bY=\by^{(h)}$ are obtained.
For the purpose of generalising from independent samples to complex samples, suppose that each unit $h$ in the sample is assigned a (normalised) survey weight $w_h$ with $\sum_h w_h = n$.
Of course, if an independent simple random sampling scheme is implemented, then each $w_h=1$.

Let $p_r = \hat n_r/n$ be the $r$th entry of the $R$-vector of proportions $\bp$, where
\begin{equation}
\hat n_r = \sum_h w_h [\by^{(h)} = \by_r],
\end{equation}
and $[\cdot]$ denotes the Iverson bracket.
In other words, $\hat n_r$ represents the (weighted) frequency counts of the observed response patterns such that $\sum_{r=1}^R \hat n_r = n$.
The vector $\hat\bn = (\hat n_1, \dots, \hat n_R)^\top$ defines a *multivariate binomial distribution*, or more commonly called a multinomial distribution with parameter $n$, $R$, and $\bpi(\btheta)$.
The probability mass function of $\hat\bn$ is given by
\begin{equation}
f_{\hat\bn}(x_1,\dots,x_R) = N! \prod_{r=1}^R \frac{1}{x_r!} \big[\pimod{r}\big]^{x_r},
\end{equation}
and the log-likelihood for $\btheta$ given the observed frequencies $\hat\bn$ is then
\begin{equation}
\ell(\btheta) = \log f_{\hat\bn}(\hat n_1,\dots,\hat n_R) = \text{const.} + \sum_{r=1}^{R} \hat n_r \log \pimod{r}.
\end{equation}











It is widely known [@agresti2012categorical] that
\begin{equation}
\sqrt N (\bp - \bpi) \xrightarrow{\text D} {\N}_R (\bzero, \bSigma),
\end{equation}
where $\bSigma = \bD - \bpi\bpi^\top$, with $\bD:= \diag(\bpi)$.





# References













