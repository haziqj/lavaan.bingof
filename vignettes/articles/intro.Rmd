---
title: "Introduction"
output:
  bookdown::html_document2:
    toc: no
    toc_depth: -1
    number_sections: FALSE
link-citations: yes
bibliography: refs.bib
pkgdown:
  as_is: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE,
  cache = TRUE
)
library(lavaan.bingof)
library(lavaan)
library(semPlot)
library(tidyverse)
theme_set(theme_bw())
```

```{r, echo = FALSE, results = "asis"}
# LaTeX shortcuts 
cat(readr::read_file("maths_shortcuts.tex"))
``` 

<!-- Extra LaTeX commands -->
\newcommand{\pimod}[1]{\pi_{#1}(\btheta)}
\newcommand{\Sigmaystar}{\bSigma_{\by^*}}
\newcommand{\pl}{\operatorname{\ell_P}}
\newcommand{\mlepl}{\hat\btheta_{\text{PL}}}
\newcommand{\mle}{\hat\btheta_{\text{ML}}}
\newcommand{\pimodpl}{\pi_{y_iy_j}^{(ij)}(\btheta)}
<!-- Extra LaTeX commands -->

Binary categorical data, where outcomes are represented as binary indicators or responses, are ubiquitous in many fields of study, including psychology, social sciences, education, finance, marketing, and beyond. 
For example, in psychology, binary categorical data may arise from questionnaires or surveys assessing traits, behaviors, or psychological disorders. 
Similarly, in social sciences, education, finance, and marketing, binary data can be used to represent attitudes, behaviors, preferences, or other categorical outcomes. 

Let $\bY = (Y_1, \dots, Y_p)^\top \in \{0,1\}^p$ be a vector of Bernoulli random variables. 
Consider a response pattern $\by = (y_1,\dots,y_p)^\top$, where each $y_i\in\{0,1\}$.
The probability of observing such a response pattern is given by the joint distribution
\begin{align*}
\pi
&= \Pr(\bY = \by)  = \Pr(Y_1=y_1,\dots,Y_p=y_p).
(\#eq:jointprob1)
\end{align*}
Note that there are a total of $R=2^p$ possible joint probabilities $\pi_r$ corresponding to all possible two-way response patterns $\by_r$.

Analyzing binary categorical data is essential for understanding the underlying structure, relationships, and patterns within the data.
Statistical models are often employed as a tool for extracting meaningful information from such data.
When we consider a parametric model with parameter vector $\btheta\in\bbR^m$, we write $\pi_r(\btheta)$ to indicate each joint probability, and 
\begin{equation}
\bpi(\btheta) = \begin{pmatrix}
\pimod{1} \\
\vdots \\
\pimod{R}  \\
\end{pmatrix} \in [0,1]^R
(\#eq:jointprob2)
\end{equation}
for the vector of joint probabilities, with $\sum_{r=1}^R \pimod{r} =1$.
We are particularly interested in *factor models*, which rely on model-based probabilities to make inferences about underlying latent variables. 

```{r sem_path}
get_fit <- function(mn = 4) {
  mod <- txt_mod(mn)
  dat <- gen_data_bin(mn, seed = 197)
  sem(mod, dat, std.lv = TRUE)
}
fit <- get_fit(4)

# graph_sem(model = fit,) + scale_y_reverse()
# lavaanPlot(fit, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey", fontname = "Helvetica"), coefs = TRUE, covs = TRUE)
semPaths(fit, intercepts = FALSE, residuals = TRUE, whatLabels = "est",        
         manifests = paste0("y", 10:1), 
         latents = paste0("eta", 1:2),
         node.width = 0.9, nCharNodes = 0, sizeMan = 8, sizeLat = 9, 
         edge.label.cex = 0.8, ThreshAtSide = FALSE, thresholdSize = 1,
         thresholdColor = "red3", rotation = 3)
```

## Assessing model fit {.tabset}

Assessing the goodness of fit of these models to the data can be challenging. 
Traditional tests, such as the Pearson chi-square test or the likelihood ratio $G$ test, may break down when dealing with large number of items, as the number of categories grows exponentially, leading to sparse data.
In particular, when conducting goodness-of-fit tests for binary factor models, low cell counts in the ensuing contingency table can result in unstable or unreliable test statistics^[In the context of contingency tables, when cell counts are low, the observed frequencies in the cells may not follow a normal distribution, as they can be influenced by random variation and may not stabilize even as the sample size increases.], leading to inaccurate assessments of model fit. 
We illustrate this problem of sparsity with a plot of observed vs expected response pattern probabilities, highlighting the limitations of traditional tests.
The following plots are for a hypothetical situation in which $p=10$ binary items (sample size $n=1000$) are analysed using a factor model.

### Full information

```{r sem_sparsity1}
# fit <- get_fit(4)
lavTables(fit, dimension = 0) %>%
  filter(X2 <= 100) %>%
  suppressWarnings() %>%
  ggplot(aes(obs.prop, est.prop, size = obs.freq, fill = X2)) +
  geom_point(alpha = 0.75, shape = 21) +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_log10() +
  scale_y_log10() +
  scale_fill_viridis_c(option = "C") +
  labs(x = "Observed proportions", y = "Expected proportions", 
       size = "Observed\nfrequency", fill = expression(chi^2~"statistic")) +
  guides(size = guide_legend(order = 2),
         shape = guide_legend(order = 1))
```

### Limited information

```{r sem_sparsity2}
bind_rows(
  lavTables(fit, dimension = 1) %>% filter(rhs == 1) %>% mutate(type = 1),
  lavTables(fit, dimension = 2) %>% filter(row == 2 & col == 2) %>% mutate(type = 2)
) %>%
  mutate(type = factor(type, labels = c("Univariate", "Bivariate"))) %>%
  suppressWarnings() %>%
  ggplot(aes(obs.prop, est.prop, size = obs.freq, fill = X2, shape = type)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0) +
  scale_shape_manual(values = c(23, 21)) +
  scale_fill_viridis_c(option = "C", ) +
  labs(x = "Observed proportions", y = "Expected proportions", shape = "Order",
       size = "Observed\nfrequency", fill = expression(chi^2~"statistic")) +
  guides(size = guide_legend(ncol = 2, order = 2),
         shape = guide_legend(order = 1))
```

## Limited information tests

To address this issue, we propose a solution based on *limited information tests*.
Limited information goodness-of-fit (LIGOF) tests are so named because they utilize a subset of information from the data to assess the goodness-of-fit of a model. 
For our project, we focus on lower-order marginals such as univariate and bivariate marginal positive probabilities, rather than the full contingency table of observed response patterns. 
This subset of information, or limited information, is then used to assess how well the model fits the observed data.

The term "limited information" reflects the fact that LIGOF tests do not rely on the full contingency table, which can become computationally challenging and statistically problematic when dealing with large numbers of categories or sparse data. 
By using a subset of information from the data that is less sensitive to low cell counts, LIGOF tests provide a computationally efficient and statistically robust approach for assessing the goodness-of-fit of a binary factor model. 

## Composite likelihood estimation








- Model-based probabilities e.g. factor models.

- How can we ensure that the model is a good fit to the data? Usual (Pearson) chi-square test or the likelihood ratio $G$ test. 

- Problem is, it breaks down when number of items is large, because the number of categories grows exponentially. A lot of these categories will end up being empty, and the above tests depends on a local CLT convergence in each category, which is not achieved. This is the problem of sparsity. Illustrate with a plot of observed vs expected reponse pattern probabilities.

- Solution: Consider lower order marginals instead: Consider how well the model fits the univariate and bivariate marginal positive probabilities. These are called limited information tests. 

- When $p$ is large, then possibly ML methods also struggle computationally. We explore LIGOF tests under a composite likelihood estimation, which provides a computational advantage. Furthermore, the LIGOF tests can be built organically from the quantities used in the PL estimation framework.

- Contributions: theory of the test statistics derivation, incl. its distribution. We ran type I and power simulations on various sample sizes. We also extend the results from independent sampling to complex sampling designs, where the estimates and test statistics are weight-adjusted.

- Conclusions...











