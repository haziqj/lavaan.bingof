---
title: "Proofs"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    number_sections: FALSE
link-citations: yes
bibliography: refs.bib
# nocite: |
#   @maydeu2008overview, @reiser1996analysis, @bartholomew2002goodness, @katsikatsou2012pairwise
pkgdown:
  as_is: true
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(lavaan)
library(lavaan.bingof)
```

```{r, echo = FALSE, results = "asis"}
# LaTeX shortcuts 
cat(readr::read_file("maths_shortcuts.tex"))
``` 

<!-- Extra LaTeX commands -->
\newcommand{\pimod}[1]{\pi_{#1}(\btheta)}
\newcommand{\Sigmaystar}{\bSigma_{\by^*}}
\newcommand{\pl}{\operatorname{\ell_P}}
\newcommand{\mlepl}{\hat\btheta_{\text{PL}}}
\newcommand{\mle}{\hat\btheta_{\text{ML}}}
\newcommand{\pimodpl}{\pi_{y_iy_j}^{(ij)}(\btheta)}
\newcommand{\tr}{\operatorname{tr}}
<!-- Extra LaTeX commands -->

## Maximum likelihood

::: {.proposition}
For the multivariate Bernoulli model in the response pattern representation with log-likelihood given by
\begin{equation}
\ell(\btheta) = \sum_{r=1}^{R} \hat n_r \log \pimod{r},
\end{equation}
the expected (unit) Fisher information matrix about the $m$-dimensional real parameter vector $\btheta$ is $\cI = \bDelta^\top \bD^{-1} \bDelta \in \bbR^{m\times m}$, where

- $\bDelta_{r,k} = \frac{\partial\pi_r(\btheta)}{\partial\theta_k}$, $r=1,\dots,R$, $k=1,\dots,m$; and
- $\bD = \diag(\pi_1(\btheta),\dots,\pi_R(\btheta))$.

:::

::: {.proof}
For $k=1,\dots,m$, the partial derivative of the log-likelihood $\ell(\btheta)$ with respect to $\theta_k$ is
\begin{equation}
\frac{\partial\ell(\btheta)}{\partial\theta_k} 
= \sum_{r=1}^{R} \hat n_r \frac{\partial\log \pimod{r}}{\partial\theta_k}
= \sum_{r=1}^{R}  \frac{\hat n_r}{\pimod{r}} \frac{\partial \pimod{r}}{\partial\theta_k}. (\#eq:derloglik)
\end{equation}
<!-- Evidently, we may write the score function as -->
<!-- \begin{equation} -->
<!-- \nabla \ell(\btheta) = \bDelta^\top \bD^{-1} \hat\bn \in \bbR^q. -->
<!-- \end{equation} -->
<!-- As $\hat\bn^\top = (\hat n_1,\dots,\hat n_R)$ represents a multivariate binomial vector, we have that $\E \hat\bn = n\bpi(\btheta)$ and $\Var(\hat\bn) = n(\bD - \pimod{}\pimod{}^\top)$. -->
Differentiating again with respect to $\theta_l$ this time, where $l\in\{1,\dots,m\}$, we get
\begin{equation}\label{eq:der_score}
\frac{\partial\ell(\btheta)}{\partial\theta_k\partial\theta_l} 
= \sum_{r=1}^{R}  \frac{\hat n_r}{\pimod{r}} \frac{\partial^2 \pimod{r}}{\partial\theta_k\partial\theta_l} - \sum_{r=1}^{R}  \frac{\hat n_r}{\pimod{r}^2} \frac{\partial \pimod{r}}{\partial\theta_k} \frac{\partial \pimod{r}}{\partial\theta_l}.
\end{equation}
Taking negative expectations of the quantity above yields the $(k,l)$th element of the **full** Fisher information matrix:
\begin{align}\label{eq:negexpscore}
-\E\left[\frac{\partial\ell(\btheta)}{\partial\theta_k\partial\theta_l}  \right] 
&= \sum_{r=1}^{R}  \frac{\E (\hat n_r)}{\pimod{r}^2} \frac{\partial \pimod{r}}{\partial\theta_k} \frac{\partial \pimod{r}}{\partial\theta_l}  - \sum_{r=1}^{R}  \frac{\E (\hat n_r)}{\pimod{r}} \frac{\partial^2 \pimod{r}}{\partial\theta_k\partial\theta_l}  \nonumber  \\
&=  n\sum_{r=1}^{R}  \frac{\cancel{\pimod{r}}}{\pimod{r}^{\cancel{2}}} \frac{\partial \pimod{r}}{\partial\theta_k} \frac{\partial \pimod{r}}{\partial\theta_l} -  n\sum_{r=1}^{R}  \frac{\cancel{\pimod{r}}}{\cancel{\pimod{r}}} \frac{\partial^2 \pimod{r}}{\partial\theta_k\partial\theta_l} \nonumber \\
&= n\sum_{r=1}^{R}  \frac{1}{\pimod{r}} \frac{\partial \pimod{r}}{\partial\theta_k} \frac{\partial \pimod{r}}{\partial\theta_l} -    \cancel{ n\sum_{r=1}^{R} \frac{\partial^2 \pimod{r}}{\partial\theta_k\partial\theta_l} } ,
\end{align}
where the cancellation of the second term in the last line above follows again from the fact that for all $k=1,\dots,m$,
\begin{equation}
\sum_{r=1}^R \pimod{r} = 1 \ \ \Rightarrow \ \  \sum_{r=1}^R \frac{\partial \pimod{r}}{\partial\theta_k}  = 0.  (\#eq:sumderzero)
\end{equation}
Dividing by $n$ gives the desired result.


<!-- Clearly, \eqref{eq:negexpscore} is the entry of the $k$th row and $j$th column of the matrix $\cI(\btheta) = N \bDelta^\top \bD^{-1} \bDelta^\top$ found earlier. -->





:::


::: {.proposition}
The maximum likelihood estimators are a class of *best asymptotically normal* (BAN) estimators $\hat\btheta$ of $\btheta$ that satisfy
\begin{equation}
\sqrt n (\hat\btheta - \btheta) = \sqrt n \, \bB \big(\bp - \bpi(\btheta)\big) + o(1),
(\#eq:ban)
\end{equation}
where $\bB = \cI^{-1} \bDelta^\top \bD^{-1}$ is an $m\times R$ matrix.
:::

::: {.proof}
Let $\hat\btheta$ be the MLE of $\btheta$.
Then, maximum likelihood theory tells us that as $n\to\infty$,
\begin{equation}\label{eq:limitdisttheta}
\sqrt n (\hat\btheta - \btheta) \xrightarrow{\text D} \N(\bzero, \cI^{-1}).
\end{equation}
Consider now the first order Taylor expansion of the score vector $\nabla\ell(\btheta)$ (with entries given in \@ref(eq:derloglik) above) about some parameter value $\btheta_0$:
\begin{align}
\nabla \ell (\btheta) 
&= \nabla \ell(\btheta_0) + 
\textcolor{gray}{\frac{n}{n}} \nabla^2\ell(\btheta_0) 
(\btheta - \btheta_0 ) + o(n^{-1/2})  \\
&\xrightarrow{\text P} \nabla \ell(\btheta_0) - n\cI (\btheta - \btheta_0)   \ \text{as $n\to\infty$}.
\end{align}
This implies that
\begin{align}
\cancelto{0}{\nabla \ell (\hat\btheta)} 
&\approx \nabla \ell(\btheta) + n\cI (\btheta - \hat\btheta)  \\
\Rightarrow (\hat\btheta-\btheta) &= \frac{1}{n} \cI^{-1} \nabla\ell(\btheta) \nonumber \\
\Rightarrow \sqrt n (\hat\btheta-\btheta) &= \frac{\sqrt n}{n} \cI^{-1} \nabla\ell(\btheta) (\#eq:taylorscore)
\end{align}

We wish to express the score vector $\nabla\ell(\btheta)$ in terms of the *errors* $\be := \bp - \bpi(\btheta)$.
Notice that
\begin{align*}
n \sum_{r=1}^{R} \big(p_r - \pimod{r}\big) \frac{1}{\pimod{r}}\frac{\partial\pimod{r}}{\partial\theta_k}
&= \sum_{r=1}^{R}  \frac{\overbrace{n p_r}^{\hat n_r}}{\pimod{r}}\frac{\partial\pimod{r}}{\partial\theta_k}
-
n \sum_{r=1}^{R}  \frac{\pimod{r}}{\pimod{r}}\frac{\partial\pimod{r}}{\partial\theta_k}
\\
&= \frac{\partial\ell(\theta)}{\partial\theta_k} 
-
\cancel{ n \sum_{r=1}^{R}  \frac{\partial\pimod{r}}{\partial\theta_k}},
\end{align*}
where the cancellation is again due to \@ref(eq:sumderzero).
We can now see how to write the score vector in terms of the residuals:
\begin{equation} 
\nabla\ell(\btheta) = n \bDelta^\top \bD^{-1} \big(\bp - \bpi(\btheta)\big). (\#eq:scoreaserrors)
\end{equation}
Substituting \@ref(eq:scoreaserrors) into \@ref(eq:taylorscore) gives us
\begin{align}
\sqrt n (\hat\btheta-\btheta) 
&\approx \sqrt n \ \cI^{-1} \bDelta^\top \bD^{-1} \ \big(\bp - \bpi(\btheta)\big) (\#eq:subB).
\end{align}
:::

::: {.proposition}
The covariance matrix of the residuals, $\bOmega = (\bI - \bDelta\bB)\bSigma(\bI - \bDelta\bB)^\top$ simplifies to $\bSigma - \bDelta\cI^{-1}\bDelta^\top$.
:::

::: {.proof}
\begin{align}
\bOmega 
&= \bSigma - \bDelta\bB\bSigma - \bSigma\bB^\top\bDelta^\top + \bDelta\bB\bSigma\bB^\top\bDelta^\top \nonumber \\
&= \bSigma - \bDelta  \cI^{-1} \bDelta^\top \bD^{-1} (\bD - \bpi(\btheta)\bpi(\btheta)^\top) - \bSigma\bB^\top\bDelta^\top + \bDelta\bB\bSigma\bB^\top\bDelta^\top \nonumber \\
&= \bSigma - \bDelta  \cI^{-1} \bDelta^\top  - 
\cancel{\bDelta  \cI^{-1} \bDelta^\top\bD^{-1} \bpi(\btheta)\bpi(\btheta)^\top} 
- \bSigma\bB^\top\bDelta^\top + \bDelta\bB\bSigma\bB^\top\bDelta^\top \nonumber \\
&= \bSigma - \bDelta  \cI^{-1} \bDelta^\top - \bDelta \cI^{-1} \bDelta^\top +  \bDelta \cI^{-1} \overbrace{ \bDelta^\top \bD^{-1}\bDelta}^{\cI} \cI^{-1} \bDelta^\top \nonumber \\
&=  \bSigma - \bDelta  \cI^{-1} \bDelta^\top.
\end{align}

The cancellation occurs because 
$$
\bDelta^\top\bD^{-1} \bpi(\btheta) = \bDelta^\top \bone = \bzero.
$$
:::


## Pairwise likelihood

::: {.proposition}
The pairwise likelihood estimator $\mlepl$ satisfies the BAN requirement \ref(eq:ban) where the $\bB$ matrix is given by $\bB = \cH^{-1}\tilde\bDelta^{-1}\tilde \bD^{-1} \bG$, with 

- $\tilde\bDelta \in \bbR^{\tilde R \times m}$ consists of partial derivatives of the pairwise probabilities, i.e. $\frac{\partial\pimodpl}{\partial\theta_k}$;
- $\tilde\bD = \diag((\pimodpl)_{i<j})$; and 
- $\bG$ is some indicator matrix to transform the quantities from $\tilde R$ dimensions to $R$ dimensions. 
:::  

::: {.proof}
The steps to show this is the same as in the maximum likelihood case, except we are now using pairwise quantities and the (unit) sensitivity matrix $\cH=-n^{-1}\E\nabla^2\pl(\btheta)$.
Thus, we may arrive at the following line 
\begin{equation}
\nabla\pl(\btheta) = n \tilde\bDelta^\top \tilde\bD^{-1} \big(
\myoverbrace{ \tilde  \bp - \tilde \bpi(\btheta) }{\tilde\be}\big). (\#eq:scorewithpairwiseresid)
\end{equation}
by following the ML steps above to arrive at \ref(eq:scoreaserrors).
The tilde here indicates that we are dealing with the $\tilde R$ pairwise quantities, negative and positive outcomes alike.
We need an indicator matrix $\bG$ of an appropriate size to transform the pairwise errors $\tilde\be \in \bbR^{\tilde R}$ appearing in the right-hand side of \@ref(eq:scorewithpairwiseresid) to the joint-model errors $\be \in \bbR^R$.
In other words, $\bG$ should consist of stacked submatrices $\bG_{y_iy_j}^{(ij)}$ that satisfy
$$
\bG_{y_iy_j}^{(ij)} 
\big(\bp - \bpi(\btheta)\big) = 
\begin{pmatrix}
p_{00}^{(ij)} - \pi(\btheta)_{00}^{(ij)}\\
p_{10}^{(ij)} - \pi(\btheta)_{10}^{(ij)}\\
p_{01}^{(ij)} - \pi(\btheta)_{01}^{(ij)}\\
p_{11}^{(ij)} - \pi(\btheta)_{11}^{(ij)}\\
\end{pmatrix}
$$
for all pairs $i,j=1,\dots,p$, $i<j$, with the added condition that the sum of 4 entries is equal to 0 (because the proportions and probabilities must sum to 1).
Such a matrix $\bG$ is not difficult to find, but the derivation can be a bit tedious, so is omitted for brevity (however, an example is given next).
Finally, we have 
\begin{equation}
\sqrt n (\mlepl - \btheta) \approx \sqrt n \ \myoverbrace{\cH^{-1}  \tilde\bDelta^\top \tilde\bD^{-1}\bG }{\bB} 
\big (  \bp - \bpi(\btheta) \big). (\#eq:mleplminusthetaBmatrix)
\end{equation}
:::    


::: {.example}
Consider $p=3$. 
Then $\tilde R = 4 \times {3 \choose 2 } = 12$ (the total number of pairwise probabilities).
All pairwise outcomes can be tabulated as below.
We are interested in obtaining the final column from the joint errors $\be = (\bp - \bpi)$.

|   | $(i,j)$ | $Y_i$ | $Y_j$ | $\tilde e_{y_i,y_j}^{(ij)}$ |
|---|-------|-------|-------|------------------------|
| 1 | (1,2) | 0     | 0     | $p_{00}^{(12)} - \pi_{00}^{(12)}$      |
| 2 | (1,2) | 1     | 0     | $p_{10}^{(12)} - \pi_{10}^{(12)}$      |
| 3 | (1,2) | 0     | 1     | $p_{01}^{(12)} - \pi_{01}^{(12)}$      |
| 4 | (1,2) | 1     | 1     | $p_{11}^{(12)} - \pi_{11}^{(12)}$      |
| 5 | (1,3) | 0     | 0     | $p_{00}^{(13)} - \pi_{00}^{(13)}$      |
| 6 | (1,3) | 1     | 0     | $p_{10}^{(13)} - \pi_{10}^{(13)}$      |
| 7 | (1,3) | 0     | 1     | $p_{01}^{(13)} - \pi_{01}^{(13)}$      |
| 8 | (1,3) | 1     | 1     | $p_{11}^{(13)} - \pi_{11}^{(13)}$      |
|9  | (2,3) | 0     | 0     | $p_{00}^{(23)} - \pi_{00}^{(23)}$      |
|10 | (2,3) | 1     | 0     | $p_{10}^{(23)} - \pi_{10}^{(23)}$      |
|11 | (2,3) | 0     | 1     | $p_{01}^{(23)} - \pi_{01}^{(23)}$      |
| 12 | (2,3) | 1     | 1     | $p_{11}^{(23)} - \pi_{11}^{(23)}$      |

For the cases '10', '01' and '11', these are straightforward. 
Example:

- $\tilde e_{10}^{(12)}$ is obtained from $(p_{100} + p_{101}) - (\pi_{100} + \pi_{101})$
  - $B_{10}^{(12)} (\bp - \bpi) = (00110000)(\bp - \bpi)$
- $\tilde e_{01}^{(12)}$ is obtained from $(p_{010} + p_{011}) - (\pi_{010} + \pi_{011})$
  - $B_{01}^{(12)} (\bp - \bpi) = (00001100)(\bp - \bpi)$
- $\tilde e_{11}^{(12)}$ is obtained from $(p_{110} + p_{111}) - (\pi_{110} + \pi_{111})$
  - $B_{11}^{(12)} (\bp - \bpi) = (11000000)(\bp - \bpi)$.

For the remaining case of '00', notice that
\begin{align*}
\tilde e_{00}^{(12)}
&= p_{00}^{(12)} - \pi_{00}^{(12)} \\
&=  (1 - p_{10}^{(12)} - p_{01}^{(12)} - p_{11}^{(12)}) - (1 - \pi_{10}^{(12)} - \pi_{01}^{(12)} - \pi_{11}^{(12)}) \\
&= - B_{10}^{(12)} (\bp - \bpi) - B_{01}^{(12)} (\bp - \bpi) - B_{11}^{(12)} (\bp - \bpi) \\
&= - (\myunderbrace{B_{10}^{(12)} + B_{01}^{(12)} + B_{11}^{(12)}}{B_{00}^{(12)}} )  (\bp - \bpi) 
\end{align*}

Following the pattern above, we should be getting the following design matrix:

|        | $e_{111}$  |  $e_{110}$ | $e_{101}$  | $e_{100}$  |  $e_{011}$ | $e_{010}$  |  $e_{001}$ | $e_{000}$  |
|-------:|---:|---:|---:|---:|---:|---:|---:|---:|
|$\tilde e_{00}^{(12)}$|  -1|  -1|  -1|  -1|  -1|  -1| 0|  0|
|$\tilde e_{10}^{(12)}$|  0|  0|  1|  1|  0|  0|  0|  0|
|$\tilde e_{01}^{(12)}$|  0|  0|  0|  0|  1|  1|  0|  0|
|$\tilde e_{11}^{(12)}$|  1|  1|  0|  0|  0|  0|  0|  0|
|$\tilde e_{00}^{(13)}$|  -1|  -1|  -1|  -1|  -1|  0|  -1|  0|
|$\tilde e_{10}^{(13)}$|  0|  1|  0|  1|  0|  0|  0|  0|
|$\tilde e_{01}^{(13)}$|  0|  0|  0|  0|  1|  0|  1|  0|
|$\tilde e_{11}^{(13)}$|  1|  0|  1|  0|  0|  0|  0|  0|
|$\tilde e_{00}^{(23)}$|  -1|  -1|  -1|  0|  -1|  -1|  -1|  0|
|$\tilde e_{10}^{(23)}$|  0|  1|  0|  0|  0|  1|  0|  0|
|$\tilde e_{01}^{(23)}$|  0|  0|  1|  0|  0|  0|  1|  0|
|$\tilde e_{11}^{(23)}$|  1|  0|  0|  0|  1|  0|  0|  0|

This is implemented in the R function `create_G_mat()`:

```{r}
create_G_mat(p = 3)
```

A couple of notes:

- In the manuscript, we describe the matrix $B$ which is a design matrix that transforms the pairwise residuals $\tilde\be$ to the unviariate and bivariate  residuals $\be_2$.
- The relationship between $\bG$, $\bT_2$ and $B$ is that $\bG = B \bT_2$.
- Thus, the $B$ matrix may be obtained by postmultiplying $\bG$ with the right inverse of $\bT_2$. The right inverse exists because $\bT_2$ is of full row rank.

```{r}
require(testthat)
B  <- lavaan.bingof:::Beta_mat_design(3)
G  <- lavaan.bingof::create_G_mat(3)
T2 <- lavaan.bingof::create_T2_mat(3)

# equal
G
B %*% T2
test_that("G and BT2 are equal", {
  expect_equal(G, B %*% T2, ignore_attr = TRUE)  
})


# equal
B
(altB <- round(G %*% MASS::ginv(T2), 0))
test_that("B can be created from T2 inv", {
  expect_equal(B, altB, ignore_attr = TRUE)
})
```

:::  

::: {.proposition}
The unit sensitivity matrix $\cH$ under the pairwise likelihood estimation method is $\cH = \tilde\bDelta^\top \tilde\bD^{-1} \tilde\bDelta$.
:::  

::: {.proof}
The maximum pairwise likelihood estimator $\mlepl$ solves the estimating equations $\bbR^q \ni \nabla \pl(\btheta) = \bzero$.
The score vector has entries
\begin{equation} 
\frac{\partial\!\pl(\btheta)}{\partial\theta_k} =
\sum_{i<j} \sum_{y_i}\sum_{y_j}  \frac{\hat n_{y_iy_j}^{(ij)} }{\pimodpl} \frac{\partial \pimodpl}{\partial\theta_k} (\#eq:bivderall)
\end{equation}
for each $k=1,\dots,q$.
Differentiating again with respect to $\theta_l$ can be derived to be
\begin{equation}
\frac{\partial\!\pl(\btheta)}{\partial\theta_k\partial\theta_l}
= 
\sum_{i<j} \sum_{y_i} \sum_{y_j} 
\left\{
\frac{\hat n_{y_iy_j}^{(ij)}}{\pimodpl}
\frac{\partial^2\pimodpl}{\partial\theta_k\partial\theta_l}
- 
\frac{\hat n_{y_iy_j}^{(ij)}}{\pimodpl^2}
\frac{\partial\pimodpl}{\partial\theta_k}
\frac{\partial\pimodpl}{\partial\theta_l}\right\}
. (\#eq:secderpl)
\end{equation}
The $(k,l)$th entry of the (full) *sensitivity matrix* is the expected value of \@ref(eq:secderpl), shown to be
\begin{equation}\label{eq:sensitivitymatrix}
-\E\left[ \frac{\partial\!\pl(\btheta)}{\partial\theta_k\partial\theta_l}
 \right]
=
n \sum_{i<j} \sum_{y_i} \sum_{y_j} 
\frac{1}{\pimodpl}
\frac{\partial\pimodpl}{\partial\theta_k}
\frac{\partial\pimodpl}{\partial\theta_l},
\end{equation}
which in matrix form is $n\tilde\bDelta^\top \tilde\bD^{-1} \tilde\bDelta$.
Dividing by $n$ gives the desired result.

:::    


::: {.proposition}
The covariance matrix of the residuals, $\bOmega = (\bI - \bDelta\bB)\bSigma(\bI - \bDelta\bB)^\top$ simplifies to $\bSigma - \bDelta\cH^{-1}\bDelta^\top$ under the pairwise likelihood framework.
:::  


::: {.proof}
Consider first the matrix $\bDelta\bB\bSigma$:

\begin{align*}
\bDelta\bB\bSigma 
&= \bDelta \cH^{-1}\tilde\bDelta^{-1}\tilde \bD^{-1} \bG \bSigma  \\
&= \bDelta \cH^{-1}\tilde\bDelta^{-1}\tilde \bD^{-1} \bG (\bD - \pimod{}\pimod{}^\top) \\
&= \vdots
\end{align*}

**Actually, this might not work...**
:::    


```{r}
fit <- lavaan.bingof:::fit_facmod_pml(1, "srs", 1000)$fit
res <- lavaan.bingof:::calc_test_stuff(fit)
attach(res)
mat1 <- Omega2
mat2 <- Sigma2 - Delta2 %*% H_inv %*% t(Delta2)
detach(res)
plot(mat1, mat2)
```



## References
